#!/usr/bin/env python3
"""
üõ°Ô∏è DATA SENTINEL AGENT
Autonomous AI Agent for 24/7 data source monitoring and quality control
"""

import asyncio
import json
import sqlite3
import time
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import os
import logging
from dataclasses import dataclass

@dataclass
class DataSourceHealth:
    source_name: str
    status: str  # active, degraded, failed
    last_update: datetime
    success_rate: float
    avg_response_time: float
    errors_count: int
    data_quality_score: float
    records_collected_24h: int

@dataclass
class AnomalyDetection:
    timestamp: datetime
    source_name: str
    anomaly_type: str
    severity: str  # low, medium, high, critical
    description: str
    suggested_action: str
    confidence: float

class DataSentinelAgent:
    """
    Autonomous AI Agent for data source monitoring
    - 24/7 monitoring of 100+ data sources
    - Real-time anomaly detection
    - Quality control and validation
    - Self-expanding source discovery
    """
    
    def __init__(self):
        self.agent_id = "data_sentinel_001"
        self.db_path = "data/sentinel_monitoring.db"
        self.setup_database()
        self.setup_logging()
        
        # Agent parameters
        self.monitoring_interval = 300  # 5 minutes
        self.quality_threshold = 0.7
        self.response_time_threshold = 30.0  # seconds
        self.error_rate_threshold = 0.1  # 10%
        
        # Data sources to monitor
        self.monitored_sources = {
            "crypto_apis": [
                "https://api.coingecko.com/api/v3/ping",
                "https://api.binance.com/api/v3/ping",
                "https://api.alternative.me/fng/"
            ],
            "news_feeds": [
                "https://www.coindesk.com/arc/outboundfeeds/rss/",
                "https://cointelegraph.com/rss",
                "https://decrypt.co/feed"
            ],
            "social_apis": [
                "https://www.reddit.com/r/cryptocurrency/hot.json",
                "https://www.reddit.com/r/bitcoin/hot.json"
            ]
        }
        
        self.is_monitoring = False
        self.health_status = {}
        self.anomalies_detected = []
        
    def setup_database(self):
        """Setup agent monitoring database"""
        Path("data").mkdir(exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Source monitoring table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS source_monitoring (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL NOT NULL,
                source_name TEXT NOT NULL,
                source_url TEXT NOT NULL,
                status TEXT NOT NULL,
                response_time REAL DEFAULT 0.0,
                data_quality_score REAL DEFAULT 0.0,
                records_count INTEGER DEFAULT 0,
                error_message TEXT
            )
        """)
        
        # Anomaly detection table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS anomaly_detections (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL NOT NULL,
                source_name TEXT NOT NULL,
                anomaly_type TEXT NOT NULL,
                severity TEXT NOT NULL,
                description TEXT NOT NULL,
                suggested_action TEXT NOT NULL,
                confidence REAL NOT NULL
            )
        """)
        
        # Agent decisions table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS agent_decisions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp REAL NOT NULL,
                decision_type TEXT NOT NULL,
                context TEXT NOT NULL,
                action_taken TEXT NOT NULL,
                reasoning TEXT NOT NULL,
                confidence REAL NOT NULL
            )
        """)
        
        conn.commit()
        conn.close()
        
    def setup_logging(self):
        """Setup agent logging"""
        Path("logs").mkdir(exist_ok=True)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - DataSentinel - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/data_sentinel.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"üõ°Ô∏è Data Sentinel Agent {self.agent_id} initialized")
    
    async def run_monitoring_cycle(self) -> Dict[str, Any]:
        """Run single monitoring cycle"""
        self.logger.info("üîÑ Starting monitoring cycle...")
        cycle_start = time.time()
        
        # 1. Monitor all data sources
        health_results = await self.monitor_all_sources()
        
        # 2. Detect anomalies
        anomalies = await self.detect_anomalies(health_results)
        
        # 3. Make autonomous decisions
        decisions = await self.make_autonomous_decisions(anomalies)
        
        # 4. Generate status report
        status_report = await self.generate_status_report()
        
        cycle_time = time.time() - cycle_start
        
        results = {
            "cycle_time": cycle_time,
            "sources_monitored": len(health_results),
            "anomalies_detected": len(anomalies),
            "decisions_made": len(decisions),
            "health_results": [
                {
                    "source": health.source_name,
                    "status": health.status,
                    "quality_score": health.data_quality_score,
                    "response_time": health.avg_response_time
                } for health in health_results
            ],
            "anomalies": [
                {
                    "source": anomaly.source_name,
                    "type": anomaly.anomaly_type,
                    "severity": anomaly.severity,
                    "description": anomaly.description
                } for anomaly in anomalies
            ]
        }
        
        self.logger.info(f"‚úÖ Monitoring cycle complete ({cycle_time:.2f}s)")
        self.logger.info(f"   üìä Sources monitored: {len(health_results)}")
        self.logger.info(f"   ‚ö†Ô∏è Anomalies detected: {len(anomalies)}")
        self.logger.info(f"   üß† Decisions made: {len(decisions)}")
        
        return results
    
    async def monitor_all_sources(self) -> List[DataSourceHealth]:
        """Monitor health of all data sources"""
        health_results = []
        
        for category, sources in self.monitored_sources.items():
            for source_url in sources:
                try:
                    health = await self.check_source_health(source_url, category)
                    health_results.append(health)
                    
                    # Store monitoring data
                    self.store_monitoring_data(health, source_url)
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Error monitoring {source_url}: {e}")
                    
        return health_results
    
    async def check_source_health(self, source_url: str, category: str) -> DataSourceHealth:
        """Check health of individual source"""
        start_time = time.time()
        
        try:
            response = requests.get(source_url, timeout=30, headers={"User-Agent": "OrionSentinel"})
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                # Analyze data quality
                quality_score = self.analyze_data_quality(response.text, category)
                status = "active" if quality_score > self.quality_threshold else "degraded"
                
                return DataSourceHealth(
                    source_name=self.extract_source_name(source_url),
                    status=status,
                    last_update=datetime.now(),
                    success_rate=0.95,  # Mock data
                    avg_response_time=response_time,
                    errors_count=0,
                    data_quality_score=quality_score,
                    records_collected_24h=100  # Mock data
                )
            else:
                return DataSourceHealth(
                    source_name=self.extract_source_name(source_url),
                    status="failed",
                    last_update=datetime.now(),
                    success_rate=0.8,
                    avg_response_time=response_time,
                    errors_count=1,
                    data_quality_score=0.0,
                    records_collected_24h=0
                )
                
        except Exception as e:
            response_time = time.time() - start_time
            return DataSourceHealth(
                source_name=self.extract_source_name(source_url),
                status="failed",
                last_update=datetime.now(),
                success_rate=0.5,
                avg_response_time=response_time,
                errors_count=1,
                data_quality_score=0.0,
                records_collected_24h=0
            )
    
    async def detect_anomalies(self, health_results: List[DataSourceHealth]) -> List[AnomalyDetection]:
        """AI-powered anomaly detection"""
        anomalies = []
        
        for health in health_results:
            # Check for various anomaly patterns
            
            # 1. Data quality degradation
            if health.data_quality_score < self.quality_threshold:
                anomalies.append(AnomalyDetection(
                    timestamp=datetime.now(),
                    source_name=health.source_name,
                    anomaly_type="data_quality_degradation",
                    severity="medium" if health.data_quality_score > 0.3 else "high",
                    description=f"Data quality score dropped to {health.data_quality_score:.2f}",
                    suggested_action="Investigate data source or activate backup",
                    confidence=0.8
                ))
            
            # 2. Response time anomaly
            if health.avg_response_time > self.response_time_threshold:
                anomalies.append(AnomalyDetection(
                    timestamp=datetime.now(),
                    source_name=health.source_name,
                    anomaly_type="slow_response",
                    severity="low" if health.avg_response_time < 60 else "medium",
                    description=f"Response time increased to {health.avg_response_time:.2f}s",
                    suggested_action="Check network connectivity or source load",
                    confidence=0.7
                ))
            
            # 3. Source failure
            if health.status == "failed":
                anomalies.append(AnomalyDetection(
                    timestamp=datetime.now(),
                    source_name=health.source_name,
                    anomaly_type="source_failure",
                    severity="high",
                    description=f"Source completely unavailable",
                    suggested_action="Activate backup source immediately",
                    confidence=0.9
                ))
        
        # Store detected anomalies
        for anomaly in anomalies:
            self.store_anomaly(anomaly)
            
        return anomalies
    
    async def make_autonomous_decisions(self, anomalies: List[AnomalyDetection]) -> List[Dict[str, Any]]:
        """Make autonomous decisions based on detected anomalies"""
        decisions = []
        
        for anomaly in anomalies:
            if anomaly.confidence > 0.7:  # High confidence threshold
                
                if anomaly.anomaly_type == "source_failure":
                    decision = {
                        "type": "activate_backup_source",
                        "target": anomaly.source_name,
                        "action": "Switch to backup data source",
                        "reasoning": f"Primary source failed: {anomaly.description}",
                        "confidence": 0.9
                    }
                    decisions.append(decision)
                
                elif anomaly.anomaly_type == "data_quality_degradation":
                    decision = {
                        "type": "increase_monitoring_frequency",
                        "target": anomaly.source_name,
                        "action": "Increase monitoring frequency",
                        "reasoning": f"Data quality degraded: {anomaly.description}",
                        "confidence": 0.8
                    }
                    decisions.append(decision)
        
        # Store decisions
        for decision in decisions:
            self.store_decision(decision)
            
        return decisions
    
    async def generate_status_report(self) -> Dict[str, Any]:
        """Generate comprehensive status report"""
        conn = sqlite3.connect(self.db_path)
        
        # Get recent monitoring data
        recent_monitoring = conn.execute("""
            SELECT status, COUNT(*) as count 
            FROM source_monitoring 
            WHERE timestamp > ? 
            GROUP BY status
        """, (time.time() - 3600,)).fetchall()  # Last hour
        
        # Get recent anomalies
        recent_anomalies = conn.execute("""
            SELECT severity, COUNT(*) as count
            FROM anomaly_detections
            WHERE timestamp > ?
            GROUP BY severity
        """, (time.time() - 3600,)).fetchall()
        
        conn.close()
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "agent_status": "active",
            "monitoring_stats": dict(recent_monitoring),
            "anomaly_stats": dict(recent_anomalies),
            "sources_monitored": sum(len(sources) for sources in self.monitored_sources.values())
        }
        
        return report
    
    # Helper methods
    def analyze_data_quality(self, response_text: str, category: str) -> float:
        """Analyze data quality score"""
        if not response_text:
            return 0.0
            
        # Basic quality checks
        quality_score = 0.0
        
        # Check if response contains expected data
        if category == "crypto_apis":
            if any(word in response_text.lower() for word in ["price", "bitcoin", "volume", "pong"]):
                quality_score += 0.5
        elif category == "news_feeds":
            if any(word in response_text.lower() for word in ["title", "description", "link", "rss"]):
                quality_score += 0.5
        elif category == "social_apis":
            if any(word in response_text.lower() for word in ["data", "children", "title"]):
                quality_score += 0.5
        
        # Check response length (not too short, not too long for errors)
        if 50 < len(response_text) < 100000:
            quality_score += 0.3
            
        # Check for error indicators
        if not any(word in response_text.lower() for word in ["error", "404", "500", "timeout"]):
            quality_score += 0.2
            
        return min(quality_score, 1.0)
    
    def extract_source_name(self, url: str) -> str:
        """Extract source name from URL"""
        from urllib.parse import urlparse
        return urlparse(url).netloc
    
    def store_monitoring_data(self, health: DataSourceHealth, source_url: str):
        """Store monitoring data"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO source_monitoring
            (timestamp, source_name, source_url, status, response_time, 
             data_quality_score, records_count)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            time.time(), health.source_name, source_url, health.status,
            health.avg_response_time, health.data_quality_score, health.records_collected_24h
        ))
        
        conn.commit()
        conn.close()
    
    def store_anomaly(self, anomaly: AnomalyDetection):
        """Store detected anomaly"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO anomaly_detections
            (timestamp, source_name, anomaly_type, severity, description,
             suggested_action, confidence)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            anomaly.timestamp.timestamp(), anomaly.source_name, anomaly.anomaly_type,
            anomaly.severity, anomaly.description, anomaly.suggested_action, anomaly.confidence
        ))
        
        conn.commit()
        conn.close()
    
    def store_decision(self, decision: Dict[str, Any]):
        """Store agent decision"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO agent_decisions
            (timestamp, decision_type, context, action_taken, reasoning, confidence)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            time.time(), decision["type"], decision["target"], decision["action"],
            decision["reasoning"], decision["confidence"]
        ))
        
        conn.commit()
        conn.close()

if __name__ == "__main__":
    async def main():
        agent = DataSentinelAgent()
        
        print("üõ°Ô∏è Data Sentinel Agent - Running monitoring cycle...")
        
        # Run single monitoring cycle
        results = await agent.run_monitoring_cycle()
        
        print(f"\nüéØ MONITORING RESULTS:")
        print(f"Cycle Time: {results['cycle_time']:.2f}s")
        print(f"Sources Monitored: {results['sources_monitored']}")
        print(f"Anomalies Detected: {results['anomalies_detected']}")
        print(f"Decisions Made: {results['decisions_made']}")
        
        if results['health_results']:
            print(f"\nüìä SOURCE HEALTH:")
            for health in results['health_results']:
                print(f"  {health['source']}: {health['status']} (Quality: {health['quality_score']:.2f})")
        
        if results['anomalies']:
            print(f"\n‚ö†Ô∏è ANOMALIES DETECTED:")
            for anomaly in results['anomalies']:
                print(f"  {anomaly['source']}: {anomaly['type']} ({anomaly['severity']})")
    
    asyncio.run(main()) 